# -*- coding: utf-8 -*-
"""projectdeeplearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gaFIz3SL23W_xjz00sXO5cMz8M1SUZ0R
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from sklearn.metrics import roc_curve, auc
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, LSTM
from tensorflow.keras.layers import Dense, Dropout, TimeDistributed, GlobalAveragePooling1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

data = pd.read_csv('synthetic_iot_dataset.csv')
data.head()

data.info()

data.Anomaly.value_counts()

data.Device_ID.value_counts()

print("Number of duplicates:", data.duplicated().sum())

print(data.isnull().sum())

from sklearn.preprocessing import LabelEncoder


categorical_columns = ['Device_ID']


label_encoders = {}

for col in categorical_columns:

    data[col] = data[col].fillna(data[col].mode()[0])


    data[col] = data[col].astype(str)


    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

print("Encoding selesai. Encoder tersimpan dalam dictionary 'label_encoders'.")
print(data[categorical_columns].head())

from sklearn.model_selection import train_test_split


X = data.drop(columns=['Anomaly'])
y = data['Anomaly']


X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)


X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5,
    random_state=42,
    stratify=y_temp
)


print(f"Training set   : X_train={X_train.shape}, y_train={y_train.shape}")
print(f"Validation set : X_val={X_val.shape}, y_val={y_val.shape}")
print(f"Test set       : X_test={X_test.shape}, y_test={y_test.shape}")

from sklearn.preprocessing import MinMaxScaler
import pandas as pd


scaler = MinMaxScaler()


X_train_scaled = scaler.fit_transform(X_train)


X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_val_scaled   = pd.DataFrame(X_val_scaled, columns=X_val.columns)
X_test_scaled  = pd.DataFrame(X_test_scaled, columns=X_test.columns)


print("Train min:", X_train_scaled.min().min(), "max:", X_train_scaled.max().max())
print("Val   min:", X_val_scaled.min().min(), "max:", X_val_scaled.max().max())
print("Test  min:", X_test_scaled.min().min(), "max:", X_test_scaled.max().max())

from imblearn.over_sampling import SMOTE

# Inisialisasi SMOTE
smote = SMOTE(random_state=42)

# Terapkan hanya pada training set (jangan ke val/test!)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

print(f"Data asli: {X_train_scaled.shape}, Label distribusi: {y_train.value_counts().to_dict()}")
print(f"Data setelah SMOTE: {X_train_resampled.shape}, Label distribusi: {pd.Series(y_train_resampled).value_counts().to_dict()}")

def create_windows(X, y, window_size=10):
    X_windows = []
    y_windows = []
    for i in range(len(X) - window_size + 1):
        X_windows.append(X[i:i+window_size])
        y_windows.append(y[i+window_size-1])
    return np.array(X_windows), np.array(y_windows)

window_size = 10

X_train_w, y_train_w = create_windows(X_train_scaled, y_train.values, window_size)
X_val_w, y_val_w     = create_windows(X_val_scaled, y_val.values, window_size)
X_test_w, y_test_w   = create_windows(X_test_scaled, y_test.values, window_size)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6
)

X_train_cnn = X_train_w.reshape((X_train_w.shape[0], X_train_w.shape[1], X_train_w.shape[2], 1))
X_val_cnn   = X_val_w.reshape((X_val_w.shape[0], X_val_w.shape[1], X_val_w.shape[2], 1))
X_test_cnn  = X_test_w.reshape((X_test_w.shape[0], X_test_w.shape[1], X_test_w.shape[2], 1))

print("X_train_cnn shape:", X_train_cnn.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, LSTM
from tensorflow.keras.layers import Dense, Dropout, TimeDistributed, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model = Sequential([
    # CNN mengekstrak fitur per timestep
    TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', padding='same'),
                    input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2], 1)),
    TimeDistributed(MaxPooling1D(2)),
    TimeDistributed(BatchNormalization()),
    TimeDistributed(Conv1D(128, kernel_size=3, activation='relu', padding='same')),
    TimeDistributed(MaxPooling1D(2)),
    TimeDistributed(GlobalAveragePooling1D()),

    # LSTM untuk ketergantungan waktu
    LSTM(64, return_sequences=False),
    Dropout(0.5),

    # Output binary
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

history = model.fit(
    X_train_cnn, y_train_w,
    validation_data=(X_val_cnn, y_val_w),
    epochs=30,
    batch_size=128,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred_prob = model.predict(X_test_cnn)
y_pred = (y_pred_prob > 0.5).astype(int)

accuracy = accuracy_score(y_test_w, y_pred)
print("Akurasi test:", accuracy)
print(classification_report(y_test_w, y_pred))

cm = confusion_matrix(y_test_w, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve

# Hitung ROC-AUC
roc_auc = roc_auc_score(y_test_w, y_pred_prob)
print("ROC-AUC:", roc_auc)

# Hitung false positive rate (fpr) dan true positive rate (tpr)
fpr, tpr, thresholds = roc_curve(y_test_w, y_pred_prob)

# Plot ROC Curve
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # garis random
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()